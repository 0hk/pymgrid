{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SARSA implementation for PymGrid (class mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don'f forget to add you own path to sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/alami/OneDrive/Documents/Reinforcement Learning/MicrogridManagement/pymgrid_v2/pymgrid-master/')\n",
    "import tqdm\n",
    "sys.path.append('../')\n",
    "from pymgrid import MicrogridGenerator as MG\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import time\n",
    "#from MicroGridEnv import *\n",
    "import random \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MicroGridEnv:  Markov Decision Process modeling the microgrid dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MicroGridEnv():\n",
    "    \"\"\"\n",
    "    Markov Decision Process associated to the microgrid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            microgrid: microgrid, mandatory\n",
    "                The controlled microgrid.\n",
    "            random_seed: int, optional\n",
    "                Seed to be used to generate the needed random numbers to size microgrids.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, microgrid, seed = 0):\n",
    "        # Set seed\n",
    "        np.random.seed(seed)\n",
    "        # Microgrid\n",
    "        self.mg = microgrid\n",
    "        # State space\n",
    "        self.observation_space = self.states()\n",
    "        # Action space\n",
    "        self.action_space = [0,1,2,3]\n",
    "        # Number of states\n",
    "        self.Ns = len(self.observation_space)\n",
    "        # Number of actions\n",
    "        self.Na = len(self.action_space)\n",
    "        self.state = None\n",
    "        self.round = None\n",
    "\n",
    "        # Start the first round\n",
    "        self.reset()\n",
    "\n",
    "    # Transition function\n",
    "    def transition(self):\n",
    "        net_load = round(self.mg.load - self.mg.pv)\n",
    "        soc = round(self.mg.battery.soc,1)\n",
    "        s_ = (net_load, soc)  # next state\n",
    "        return s_\n",
    "            \n",
    "    # Reward function\n",
    "    def reward(self):\n",
    "        return -self.mg.get_cost() - self.mg.penalty(0.5)\n",
    "    \n",
    "    def step(self, action):\n",
    "        control_dict = self.get_action(action)\n",
    "        _ = self.mg.run(control_dict)\n",
    "        reward = self.reward()\n",
    "        s_ = self.transition()\n",
    "        self.state = s_\n",
    "        done = self.round == self.mg.horizon\n",
    "        self.round += 1\n",
    "        return s_, reward, done, {}\n",
    "    \n",
    "        \n",
    "    def reset(self):\n",
    "        self.round = 1\n",
    "        # Reseting microgrid\n",
    "        self.mg.reset()\n",
    "        # Building first state\n",
    "        net_load = round(self.mg.load - self.mg.pv)\n",
    "        soc = round(self.mg.battery.soc,1)\n",
    "        self.state = (net_load, soc)\n",
    "\n",
    "    # Building the observations_space from the forecast time series\n",
    "    def states(self):\n",
    "        observation_space = []\n",
    "        mg = self.mg\n",
    "        net_load = mg.forecast_load() - mg.forecast_pv()\n",
    "        for i in range(int(net_load.min()-1),int(net_load.max()+2)):\n",
    "            for j in np.arange(round(mg.battery.soc_min,1),round(mg.battery.soc_max+0.1,1),0.1):    \n",
    "                j = round(j,1)\n",
    "                observation_space.append((i,j)) \n",
    "        return observation_space\n",
    "    \n",
    "    # Mapping between action and the control_dict\n",
    "    def get_action(self, action):\n",
    "        \"\"\"\n",
    "        :param action: current action\n",
    "        :return: control_dict : dicco of controls\n",
    "        \"\"\"\n",
    "        mg = self.mg\n",
    "        pv = mg.pv\n",
    "        load = mg.load\n",
    "        net_load = load - pv\n",
    "        capa_to_charge = mg.battery.capa_to_charge\n",
    "        p_charge_max = mg.battery.p_charge_max\n",
    "        p_charge = max(0,min(-net_load, capa_to_charge, p_charge_max))\n",
    "        \n",
    "        capa_to_discharge = mg.battery.capa_to_discharge\n",
    "        p_discharge_max = mg.battery.p_discharge_max\n",
    "        p_discharge = max(0,min(net_load, capa_to_discharge, p_discharge_max))\n",
    "    \n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': 0,\n",
    "                        'battery_discharge': 0,\n",
    "                        'grid_import': 0,\n",
    "                        'grid_export':0\n",
    "                               }\n",
    "        if action == 0:\n",
    "            control_dict['battery_charge'] = p_charge*(p_charge > 0) + net_load*(p_charge <=0)  \n",
    "            control_dict['grid_export'] = max(0,pv - min(pv,load) - p_charge)\n",
    "        \n",
    "        elif action == 1:\n",
    "            control_dict['battery_discharge'] = p_discharge*(p_discharge > 0) + net_load*(p_discharge <=0)  \n",
    "            control_dict['grid_import'] = max(0,load - min(pv,load) - p_discharge)\n",
    "        \n",
    "        elif action == 2:\n",
    "            control_dict['grid_import'] = abs(net_load)\n",
    "            \n",
    "        elif action == 3:\n",
    "            control_dict['grid_export'] = abs(net_load)\n",
    "            \n",
    "        return control_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    \"\"\"\n",
    "    Implementation of SARSA algorithm.\n",
    "\n",
    "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, tau=1.0, tau_decay=0.9995,\n",
    "                 tau_min=0.25, seed=42):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.tau = tau\n",
    "        self.tau_decay = tau_decay\n",
    "        self.tau_min = tau_min\n",
    "        self.Q = np.zeros((env.Ns, env.Na))\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "        self.state = env.reset()\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "    def get_delta(self, r, x, a, y, next_a, done):\n",
    "        \"\"\"\n",
    "        :param r: reward\n",
    "        :param x: current state\n",
    "        :param a: current action\n",
    "        :param y: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        q_y_a = self.Q[y, next_a]\n",
    "        q_x_a = self.Q[x, a]\n",
    "\n",
    "        return r + self.gamma*q_y_a - q_x_a\n",
    "\n",
    "    def get_learning_rate(self, s, a):\n",
    "        if self.learning_rate is None:\n",
    "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
    "        else:\n",
    "            return max(self.learning_rate, self.min_learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q = self.Q[state, :]\n",
    "        prob = softmax(q/self.tau)\n",
    "        a = np.random.choice(self.env.action_space, p=prob)\n",
    "        return a\n",
    "\n",
    "    def step(self):\n",
    "        # Current state\n",
    "        x = self.env.state\n",
    "        x_i = self.env.observation_space.index(x)\n",
    "        # Choose action\n",
    "        a = self.get_action(x_i)\n",
    "\n",
    "        # Learning rate\n",
    "        alpha = self.get_learning_rate(x_i, a)\n",
    "\n",
    "        # Take step\n",
    "        observation, reward, done, info = self.env.step(a)\n",
    "        y = observation\n",
    "        y_i = self.env.observation_space.index(y)\n",
    "        r = reward\n",
    "        next_a = self.get_action(y_i)\n",
    "        delta = self.get_delta(r, x_i, a, y_i, next_a, done)\n",
    "\n",
    "        # Update\n",
    "        self.Q[x_i, a] = self.Q[x_i, a] + alpha*delta\n",
    "\n",
    "        self.Nsa[x_i, a] += 1\n",
    "\n",
    "        if done:\n",
    "            # print(x, observation, reward)\n",
    "            self.tau = max(self.tau*self.tau_decay, self.tau_min)\n",
    "            self.env.reset()\n",
    "        return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the SARSA algorithm on the created microgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "     Training a SARSA Policy            \n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:35<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "              Q values & greedy policy         \n",
      "-----------------------------------------------\n",
      "\n",
      "Q values (Sarsa);\n",
      " [[  -461.84826848  -4079.32286324  -6254.96172364   -445.97386941]\n",
      " [  -187.5             0.          -6254.96172364      0.        ]\n",
      " [  -287.84017204  -4079.32286324  -6423.71172364   -274.65264222]\n",
      " ...\n",
      " [-20526.76539584   -411.47901131   -411.47901131 -27431.93408732]\n",
      " [-17804.54317362   -781.81012149   -672.78028105 -27431.93408732]\n",
      " [-14401.76539584   -781.81012149   -411.47901131 -27431.93408732]]\n",
      "\n",
      "Policy (Sarsa):  [3 1 3 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mg_generator = MG.MicrogridGenerator(nb_microgrid=1)\n",
    "mg_generator.generate_microgrid(verbose = False)\n",
    "microgrid = mg_generator.microgrids[0]\n",
    "\n",
    "# Itiniation of a MicroGridEnv\n",
    "env = MicroGridEnv(microgrid = microgrid)\n",
    "\n",
    "gamma = 0.9 # Discount factor\n",
    "n_episodes = 100 # Number of episodes\n",
    "\n",
    "\"\"\"\n",
    "SARSA\n",
    "\"\"\"\n",
    "# Initiation of a qlearning object\n",
    "sarsa = Sarsa(env, gamma=gamma)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"     Training a SARSA Policy            \")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "outer = tqdm.tqdm(total = n_episodes, position=0)\n",
    "\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    outer.update(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        done = sarsa.step()\n",
    "\n",
    "        \n",
    "print(\"-----------------------------------------------\")        \n",
    "print(\"              Q values & greedy policy         \")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(\"\\nQ values (Sarsa);\\n\", sarsa.Q)\n",
    "print(\"\\nPolicy (Sarsa): \", np.argmax(sarsa.Q, axis=1))\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
